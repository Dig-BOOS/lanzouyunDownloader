#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è“å¥äº‘å•æ–‡ä»¶é“¾æ¥æ‰¹é‡ä¸‹è½½V1.0
æ”¯æŒï¼šå¹¶å‘æ§åˆ¶ã€è‡ªåŠ¨é‡è¯•ã€éšæœºå»¶æ—¶ã€è¿›åº¦æ¡ã€æ–­ç‚¹ç»­ä¼ ã€å¤±è´¥è®°å½•ã€æ–‡ä»¶åé˜²å†²çª
"""

import asyncio
import os
import random
import logging
from typing import List, Tuple

from tqdm import tqdm
from playwright.async_api import async_playwright, BrowserContext

# ==================== é…ç½®åŒºåŸŸ ====================
LINKS_FILE = "lanzou_links.txt"          # è¾“å…¥æ–‡ä»¶ï¼šæ¯è¡Œä¸€ä¸ªé“¾æ¥ï¼Œå¸¦å¯†ç ç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ https://xxx,1234
DOWNLOAD_DIR = os.path.abspath("./downloads")  # ä¸‹è½½ä¿å­˜ç›®å½•
PROCESSED_FILE = "processed.txt"         # å·²æˆåŠŸå¤„ç†çš„é“¾æ¥è®°å½•ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
FAILED_FILE = "failed.txt"               # æœ€ç»ˆå¤±è´¥çš„é“¾æ¥è®°å½•

# å¹¶å‘ä¸å»¶æ—¶æ§åˆ¶
CONCURRENCY = 3                           # åŒæ—¶å¤„ç†çš„é“¾æ¥æ•°ï¼ˆå»ºè®®1-5ï¼‰
MAX_RETRIES = 2                            # æ¯ä¸ªé“¾æ¥å¤±è´¥é‡è¯•æ¬¡æ•°
DELAY_BETWEEN_LINKS = (2, 5)                # éšæœºå»¶æ—¶èŒƒå›´ï¼ˆç§’ï¼‰

# è°ƒè¯•ä¸å¯è§†åŒ–
HEADLESS = False                           # æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆFalseå¯è§‚å¯Ÿæµè§ˆå™¨æ“ä½œï¼‰
SAVE_HTML = False                           # æ˜¯å¦ä¿å­˜æ¯ä¸ªé¡µé¢çš„HTMLï¼ˆè°ƒè¯•ç”¨ï¼Œå¤§é‡ä¸‹è½½å»ºè®®Falseï¼‰
LOG_LEVEL = logging.INFO                    # æ—¥å¿—çº§åˆ«

# ==================================================

# åˆå§‹åŒ–æ–‡ä»¶å¤¹
os.makedirs(DOWNLOAD_DIR, exist_ok=True)
if SAVE_HTML:
    os.makedirs("page_html", exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("downloader.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def get_unique_filepath(directory: str, filename: str) -> str:
    """
    å¦‚æœ filename åœ¨ directory ä¸­å·²å­˜åœ¨ï¼Œåˆ™è‡ªåŠ¨æ·»åŠ æ•°å­—åºå·è¿”å›æ–°è·¯å¾„
    ä¾‹å¦‚ï¼šæ–‡ä»¶.pdf -> æ–‡ä»¶_1.pdf -> æ–‡ä»¶_2.pdf
    """
    base, ext = os.path.splitext(filename)
    counter = 1
    filepath = os.path.join(directory, filename)
    while os.path.exists(filepath):
        new_filename = f"{base}_{counter}{ext}"
        filepath = os.path.join(directory, new_filename)
        counter += 1
    return filepath

def read_links() -> List[Tuple[str, str]]:
    """è¯»å–é“¾æ¥æ–‡ä»¶ï¼Œè¿”å›åˆ—è¡¨ [(url, password), ...]"""
    links = []
    if not os.path.exists(LINKS_FILE):
        logging.error(f"é“¾æ¥æ–‡ä»¶ {LINKS_FILE} ä¸å­˜åœ¨ï¼")
        return links
    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = line.split(",")
            url = parts[0].strip()
            pwd = parts[1].strip() if len(parts) > 1 else ""
            links.append((url, pwd))
    logging.info(f"å…±è¯»å–åˆ° {len(links)} ä¸ªé“¾æ¥")
    return links

def load_processed() -> set:
    """åŠ è½½å·²æˆåŠŸå¤„ç†çš„é“¾æ¥ï¼ˆå»é‡ï¼‰"""
    processed = set()
    if os.path.exists(PROCESSED_FILE):
        with open(PROCESSED_FILE, "r", encoding="utf-8") as f:
            processed = {line.strip() for line in f if line.strip()}
        logging.info(f"å·²ä»è®°å½•ä¸­è·³è¿‡ {len(processed)} ä¸ªå·²ä¸‹è½½é“¾æ¥")
    return processed

def save_processed(url: str):
    """è®°å½•æˆåŠŸå¤„ç†çš„é“¾æ¥"""
    with open(PROCESSED_FILE, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def save_failed(url: str, password: str):
    """è®°å½•æœ€ç»ˆå¤±è´¥çš„é“¾æ¥"""
    with open(FAILED_FILE, "a", encoding="utf-8") as f:
        if password:
            f.write(f"{url},{password}\n")
        else:
            f.write(f"{url}\n")

async def process_single_link(link_info: Tuple[str, str], context: BrowserContext, pbar: tqdm) -> bool:
    """
    å¤„ç†å•ä¸ªé“¾æ¥ï¼ˆå«é‡è¯•é€»è¾‘ï¼‰
    è¿”å› True è¡¨ç¤ºæˆåŠŸï¼ŒFalse è¡¨ç¤ºæœ€ç»ˆå¤±è´¥
    """
    url, password = link_info
    for attempt in range(MAX_RETRIES + 1):
        page = None
        try:
            # éšæœºå»¶æ—¶ï¼ˆé¦–æ¬¡å°è¯•ä¹Ÿå»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
            if attempt == 0:
                delay = random.uniform(*DELAY_BETWEEN_LINKS)
                await asyncio.sleep(delay)
            else:
                # é‡è¯•å‰ç­‰å¾…æ›´é•¿æ—¶é—´
                await asyncio.sleep(random.uniform(5, 10))

            logging.debug(f"å¼€å§‹å¤„ç†: {url} (å°è¯• {attempt+1}/{MAX_RETRIES+1})")

            # åˆ›å»ºæ–°é¡µé¢
            page = await context.new_page()
            page.set_default_timeout(60000)

            # è®¿é—®é“¾æ¥
            await page.goto(url, wait_until="networkidle")
            await page.wait_for_timeout(3000)

            # å¦‚æœæœ‰å¯†ç ï¼Œå°è¯•è¾“å…¥ï¼ˆä¸»é¡µé¢å¯èƒ½æ²¡æœ‰å¯†ç æ¡†ï¼Œå¿½ç•¥å¼‚å¸¸ï¼‰
            if password:
                try:
                    pwd_input = await page.wait_for_selector('input[type="password"]', timeout=5000)
                    await pwd_input.fill(password)
                    submit_btn = await page.wait_for_selector(
                        'button:has-text("ç¡® å®š"), button:has-text("æå–æ–‡ä»¶"), input[type="submit"]',
                        timeout=5000
                    )
                    await submit_btn.click()
                    await page.wait_for_timeout(2000)
                except Exception:
                    logging.debug("ä¸»é¡µé¢æ— å¯†ç æ¡†æˆ–å¯†ç é”™è¯¯ï¼Œå¿½ç•¥")

            # ç­‰å¾… iframe å¹¶åˆ‡æ¢åˆ°å†…éƒ¨
            iframe_element = await page.wait_for_selector('iframe.ifr2', timeout=30000)
            frame = await iframe_element.content_frame()
            if not frame:
                raise Exception("æ— æ³•è·å– iframe å†…å®¹")
            await frame.wait_for_load_state("networkidle")

            # åœ¨ iframe ä¸­å¯»æ‰¾ä¸‹è½½é“¾æ¥ï¼ˆå¸¸ç”¨é€‰æ‹©å™¨ï¼‰
            download_link = await frame.wait_for_selector(
                'a:has-text("æ™®é€šä¸‹è½½"), a:has-text("ç‚¹å‡»ä¸‹è½½"), a.download-btn, a[href*="file"], a[href*="lanrar"]',
                timeout=30000
            )

            # ç›‘å¬ä¸‹è½½å¹¶ç‚¹å‡»
            async with page.expect_download() as download_info:
                await download_link.click()
            download = await download_info.value

            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¹¶ä¿å­˜
            raw_filename = download.suggested_filename
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            clean_filename = "".join(c for c in raw_filename if c not in r'\/:*?"<>|')
            filepath = get_unique_filepath(DOWNLOAD_DIR, clean_filename)
            await download.save_as(filepath)
            logging.info(f"âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(filepath)}")

            # è®°å½•æˆåŠŸ
            save_processed(url)
            pbar.update(1)
            return True

        except Exception as e:
            logging.warning(f"âŒ å°è¯• {attempt+1}/{MAX_RETRIES+1} å¤±è´¥: {url} - {str(e)[:100]}")
            if attempt == MAX_RETRIES:  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                save_failed(url, password)
                pbar.update(1)
                return False
            # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
        finally:
            if page:
                await page.close()

    return False  # ä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ

async def main():
    # è¯»å–é“¾æ¥
    all_links = read_links()
    if not all_links:
        return

    # åŠ è½½å·²å¤„ç†è®°å½•ï¼Œè¿‡æ»¤æ‰å·²æˆåŠŸçš„
    processed = load_processed()
    remaining = [link for link in all_links if link[0] not in processed]
    logging.info(f"å‰©ä½™å¾…å¤„ç†é“¾æ¥: {len(remaining)}")

    if not remaining:
        logging.info("æ‰€æœ‰é“¾æ¥å‡å·²ä¸‹è½½å®Œæˆï¼")
        return

    # åˆå§‹åŒ–è¿›åº¦æ¡
    pbar = tqdm(total=len(remaining), desc="æ€»ä½“è¿›åº¦", unit="ä¸ª")

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(
            headless=HEADLESS,
            args=['--disable-blink-features=AutomationControlled']
        )
        context = await browser.new_context(
            accept_downloads=True,
            viewport={'width': 1280, 'height': 800},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )

        # å¹¶å‘æ§åˆ¶ï¼šä½¿ç”¨ Semaphore
        semaphore = asyncio.Semaphore(CONCURRENCY)

        async def worker(link):
            async with semaphore:
                return await process_single_link(link, context, pbar)

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [worker(link) for link in remaining]
        results = await asyncio.gather(*tasks)

        await browser.close()

    # ç»Ÿè®¡ç»“æœ
    success = sum(results)
    failed = len(results) - success
    logging.info(f"ğŸ‰ æ‰¹é‡ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {success}, å¤±è´¥: {failed}")
    if failed:
        logging.info(f"å¤±è´¥é“¾æ¥å·²è®°å½•åˆ° {FAILED_FILE}")

if __name__ == "__main__":
    asyncio.run(main())